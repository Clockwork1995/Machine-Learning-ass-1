{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 [Multiclass Perceptron, 20 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 4.0.2\"\n"
     ]
    }
   ],
   "source": [
    "library(ggplot2)\n",
    "library(reshape2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 \n",
    "##### Load Task1D_train.csv and Task1D_test.csv sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train and test data sets\n",
    "train = read.csv(\"Data_set/Task1D_train.csv\")\n",
    "train_data = train[1:4]\n",
    "train_label = train[5]\n",
    "\n",
    "test = read.csv(\"Data_set/Task1D_test.csv\")\n",
    "test_data = test[1:4]\n",
    "test_label = test[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2\n",
    "##### Implement the multiclass perceptron as explained above. Please provide enough comments for your code in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels for convinience\n",
    "encode = function (label, en=c(1, 2, 3)){\n",
    "    encoding = c()\n",
    "    for (lbl in label){\n",
    "        if (lbl == \"C1\"){ encoding = append(encoding, en[1]) }\n",
    "        else if (lbl == \"C2\"){encoding = append(encoding, en[2])}\n",
    "        else { encoding = append(encoding, en[3])}\n",
    "    }\n",
    "    return (encoding)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = function(data, W){\n",
    "    Phi = as.matrix(cbind(1, data))\n",
    "    pred = data.frame(\"prediction\"=1:nrow(data))\n",
    "    for (i in 1:nrow(data)){\n",
    "        L1 = W[[1]]%*%Phi[i,]\n",
    "        L2 = W[[2]]%*%Phi[i,]\n",
    "        L3 = W[[3]]%*%Phi[i,]\n",
    "        pred[i,] = sort(c(L1, L2, L3), index.return = TRUE, decreasing=TRUE)$ix[1]   \n",
    "    }\n",
    "    return (pred)  \n",
    "}\n",
    "\n",
    "get_missclassfications = function(prediction, label){\n",
    "    sum(prediction[,1] != encode(label[, 1])) / nrow(label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = function(train_data, train_label, test_data, test_label, eta, epsilon, tau_max){\n",
    "    \n",
    "    Phi = as.matrix(cbind(1, train_data))\n",
    "\n",
    "    # Initialize weights\n",
    "    # Weights for C1\n",
    "    W1 = matrix(,nrow=tau_max, ncol=ncol(Phi))\n",
    "    W1[1,] = runif(ncol(Phi))\n",
    "    # Weights for C2\n",
    "    W2 = matrix(,nrow=tau_max, ncol=ncol(Phi))\n",
    "    W2[1,] = runif(ncol(Phi))\n",
    "    # Weights for C3\n",
    "    W3 = matrix(,nrow=tau_max, ncol=ncol(Phi))\n",
    "    W3[1,] = runif(ncol(Phi))\n",
    "\n",
    "    # Combine under single list for better accessibility\n",
    "    W = list(W1, W2, W3)\n",
    "    \n",
    "    # Keep track for weight vectors updations\n",
    "    tau_w = c(1, 1, 1)\n",
    "    #Iteration counter\n",
    "    tau = 1\n",
    "    errors = data.frame()\n",
    "    while(!FALSE){\n",
    "\n",
    "        # resuffling train data and associated labels:\n",
    "        train_index = sample(1:nrow(train_data), replace = FALSE)\n",
    "        # Get training exaples\n",
    "        Phi = Phi[train_index,]\n",
    "        # Get train labbels and encode them for convinience\n",
    "        target = encode(train_label[train_index,])\n",
    "\n",
    "        #Training starts for each training datapoint\n",
    "        for (i in 1:nrow(train_data)){\n",
    "            # Termination condition \n",
    "            if (tau == tau_max) {break}\n",
    "\n",
    "            if (i %% 5 == 0){\n",
    "                Wt = list(W[[1]][tau_w[1],], W[[2]][tau_w[2],],W[[3]][tau_w[3],])\n",
    "                e = data.frame(\"test\"= get_missclassfications(predict(test_data, Wt), test_label))\n",
    "                errors = rbind(errors, e)\n",
    "\n",
    "            }\n",
    "\n",
    "            # Calculate logit values from weight vectors for each label\n",
    "            Logit_C1 = W[[1]][tau_w[1],]%*%Phi[i,]\n",
    "            Logit_C2 = W[[2]][tau_w[2],]%*%Phi[i,]\n",
    "            Logit_C3 = W[[3]][tau_w[3],]%*%Phi[i,]\n",
    "\n",
    "            # Get predictions. We will assign datapoint to the class with largest logit value\n",
    "            pred_idx = sort(c(Logit_C1, Logit_C2, Logit_C3), index.return = TRUE, decreasing=TRUE)$ix[1]\n",
    "\n",
    "            # Target label to match\n",
    "            target_idx = target[i]\n",
    "\n",
    "            # If missclassified\n",
    "            if (pred_idx != target_idx){\n",
    "                # Update interation (Tau)\n",
    "                tau = tau + 1\n",
    "\n",
    "                # Update the indexes in weight matrix (We'll adjust the weights)\n",
    "                tau_w[pred_idx]  = tau_w[pred_idx] + 1\n",
    "                tau_w[target_idx]  = tau_w[target_idx] + 1\n",
    "\n",
    "                # Update weight vector for missclassified class label\n",
    "                W[[pred_idx]][tau_w[pred_idx],] = W[[pred_idx]][tau_w[pred_idx]-1,] - eta * Phi[i,]\n",
    "                # Update weight vector for true class label\n",
    "                W[[target_idx]][tau_w[target_idx],] = W[[target_idx]][tau_w[target_idx]-1,] + eta * Phi[i,] \n",
    "\n",
    "            } \n",
    "        }\n",
    "        # Reduce learning rate\n",
    "        eta = eta * 0.99\n",
    "        \n",
    "        # Terminate training\n",
    "        if (tau >= tau_max){break}\n",
    "\n",
    "        W_old = list(W[[1]][tau_w[1]-1,], W[[2]][tau_w[2]-1,],W[[3]][tau_w[3]-1,])\n",
    "        W_new = list(W[[1]][tau_w[1],], W[[2]][tau_w[2],],W[[3]][tau_w[3],])\n",
    "        p_old = predict(train_data, W_old)\n",
    "        p_new = predict(train_data, W_new)\n",
    "        miss_old = get_missclassfications(p_old, train_label)\n",
    "        miss_new = get_missclassfications(p_new, train_label)\n",
    "\n",
    "        if (abs(miss_new - miss_old) <= epsilon) {break}\n",
    "\n",
    "    }\n",
    "\n",
    "    # Extract final weights\n",
    "    W1 = W[[1]][tau_w[1],]\n",
    "    W2 = W[[2]][tau_w[2],]\n",
    "    W3 = W[[3]][tau_w[3],]\n",
    "\n",
    "    # Final Weights\n",
    "    Weights = list(W1, W2, W3)\n",
    "    \n",
    "    return (list(\"W\"=Weights, \"error\"=errors)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3\n",
    "##### Train two multiclass perceptron models on the provided training data by setting the learning rates Î· to .1 and .01 respectively. Note that all parameter settings stay the same, except the learning rate, when building each model.For each model, evaluate the error of the model on the test data, after processing every 5 training data points (also known as a mini-batch). Then, plot the testing errors of two models built based on the learning rates .1 and .01(with different colors) versus the number of mini-batches. Include it in your Jupyter Notebook file for Question 6.Now, explain how the testing errors of two models behave differently, as the training data increases, by observing your plot. (Include all your answers in your Jupyter Notebook file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 30 Ã— 3</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>batch.id</th><th scope=col>eta.01</th><th scope=col>eta.10</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>0.6133333</td><td>0.36000000</td></tr>\n",
       "\t<tr><td> 2</td><td>0.5733333</td><td>0.66666667</td></tr>\n",
       "\t<tr><td> 3</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td> 4</td><td>0.4933333</td><td>0.37333333</td></tr>\n",
       "\t<tr><td> 5</td><td>0.4666667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td> 6</td><td>0.6666667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td> 7</td><td>0.6666667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td> 8</td><td>0.3466667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td> 9</td><td>0.3466667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td>10</td><td>0.3466667</td><td>0.09333333</td></tr>\n",
       "\t<tr><td>11</td><td>0.3466667</td><td>0.09333333</td></tr>\n",
       "\t<tr><td>12</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>13</td><td>0.6666667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td>14</td><td>0.6000000</td><td>0.34666667</td></tr>\n",
       "\t<tr><td>15</td><td>0.5200000</td><td>0.25333333</td></tr>\n",
       "\t<tr><td>16</td><td>0.1466667</td><td>0.34666667</td></tr>\n",
       "\t<tr><td>17</td><td>0.6266667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>18</td><td>0.2666667</td><td>0.64000000</td></tr>\n",
       "\t<tr><td>19</td><td>0.6666667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td>20</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>21</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>22</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>23</td><td>0.5866667</td><td>0.33333333</td></tr>\n",
       "\t<tr><td>24</td><td>0.3200000</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>25</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>26</td><td>0.2800000</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>27</td><td>0.6666667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>28</td><td>0.3066667</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>29</td><td>0.4133333</td><td>0.66666667</td></tr>\n",
       "\t<tr><td>30</td><td>0.6000000</td><td>0.66666667</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 Ã— 3\n",
       "\\begin{tabular}{lll}\n",
       " batch.id & eta.01 & eta.10\\\\\n",
       " <int> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  1 & 0.6133333 & 0.36000000\\\\\n",
       "\t  2 & 0.5733333 & 0.66666667\\\\\n",
       "\t  3 & 0.6666667 & 0.66666667\\\\\n",
       "\t  4 & 0.4933333 & 0.37333333\\\\\n",
       "\t  5 & 0.4666667 & 0.33333333\\\\\n",
       "\t  6 & 0.6666667 & 0.33333333\\\\\n",
       "\t  7 & 0.6666667 & 0.33333333\\\\\n",
       "\t  8 & 0.3466667 & 0.33333333\\\\\n",
       "\t  9 & 0.3466667 & 0.33333333\\\\\n",
       "\t 10 & 0.3466667 & 0.09333333\\\\\n",
       "\t 11 & 0.3466667 & 0.09333333\\\\\n",
       "\t 12 & 0.6666667 & 0.66666667\\\\\n",
       "\t 13 & 0.6666667 & 0.33333333\\\\\n",
       "\t 14 & 0.6000000 & 0.34666667\\\\\n",
       "\t 15 & 0.5200000 & 0.25333333\\\\\n",
       "\t 16 & 0.1466667 & 0.34666667\\\\\n",
       "\t 17 & 0.6266667 & 0.66666667\\\\\n",
       "\t 18 & 0.2666667 & 0.64000000\\\\\n",
       "\t 19 & 0.6666667 & 0.33333333\\\\\n",
       "\t 20 & 0.6666667 & 0.66666667\\\\\n",
       "\t 21 & 0.6666667 & 0.66666667\\\\\n",
       "\t 22 & 0.6666667 & 0.66666667\\\\\n",
       "\t 23 & 0.5866667 & 0.33333333\\\\\n",
       "\t 24 & 0.3200000 & 0.66666667\\\\\n",
       "\t 25 & 0.6666667 & 0.66666667\\\\\n",
       "\t 26 & 0.2800000 & 0.66666667\\\\\n",
       "\t 27 & 0.6666667 & 0.66666667\\\\\n",
       "\t 28 & 0.3066667 & 0.66666667\\\\\n",
       "\t 29 & 0.4133333 & 0.66666667\\\\\n",
       "\t 30 & 0.6000000 & 0.66666667\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 Ã— 3\n",
       "\n",
       "| batch.id &lt;int&gt; | eta.01 &lt;dbl&gt; | eta.10 &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "|  1 | 0.6133333 | 0.36000000 |\n",
       "|  2 | 0.5733333 | 0.66666667 |\n",
       "|  3 | 0.6666667 | 0.66666667 |\n",
       "|  4 | 0.4933333 | 0.37333333 |\n",
       "|  5 | 0.4666667 | 0.33333333 |\n",
       "|  6 | 0.6666667 | 0.33333333 |\n",
       "|  7 | 0.6666667 | 0.33333333 |\n",
       "|  8 | 0.3466667 | 0.33333333 |\n",
       "|  9 | 0.3466667 | 0.33333333 |\n",
       "| 10 | 0.3466667 | 0.09333333 |\n",
       "| 11 | 0.3466667 | 0.09333333 |\n",
       "| 12 | 0.6666667 | 0.66666667 |\n",
       "| 13 | 0.6666667 | 0.33333333 |\n",
       "| 14 | 0.6000000 | 0.34666667 |\n",
       "| 15 | 0.5200000 | 0.25333333 |\n",
       "| 16 | 0.1466667 | 0.34666667 |\n",
       "| 17 | 0.6266667 | 0.66666667 |\n",
       "| 18 | 0.2666667 | 0.64000000 |\n",
       "| 19 | 0.6666667 | 0.33333333 |\n",
       "| 20 | 0.6666667 | 0.66666667 |\n",
       "| 21 | 0.6666667 | 0.66666667 |\n",
       "| 22 | 0.6666667 | 0.66666667 |\n",
       "| 23 | 0.5866667 | 0.33333333 |\n",
       "| 24 | 0.3200000 | 0.66666667 |\n",
       "| 25 | 0.6666667 | 0.66666667 |\n",
       "| 26 | 0.2800000 | 0.66666667 |\n",
       "| 27 | 0.6666667 | 0.66666667 |\n",
       "| 28 | 0.3066667 | 0.66666667 |\n",
       "| 29 | 0.4133333 | 0.66666667 |\n",
       "| 30 | 0.6000000 | 0.66666667 |\n",
       "\n"
      ],
      "text/plain": [
       "   batch.id eta.01    eta.10    \n",
       "1   1       0.6133333 0.36000000\n",
       "2   2       0.5733333 0.66666667\n",
       "3   3       0.6666667 0.66666667\n",
       "4   4       0.4933333 0.37333333\n",
       "5   5       0.4666667 0.33333333\n",
       "6   6       0.6666667 0.33333333\n",
       "7   7       0.6666667 0.33333333\n",
       "8   8       0.3466667 0.33333333\n",
       "9   9       0.3466667 0.33333333\n",
       "10 10       0.3466667 0.09333333\n",
       "11 11       0.3466667 0.09333333\n",
       "12 12       0.6666667 0.66666667\n",
       "13 13       0.6666667 0.33333333\n",
       "14 14       0.6000000 0.34666667\n",
       "15 15       0.5200000 0.25333333\n",
       "16 16       0.1466667 0.34666667\n",
       "17 17       0.6266667 0.66666667\n",
       "18 18       0.2666667 0.64000000\n",
       "19 19       0.6666667 0.33333333\n",
       "20 20       0.6666667 0.66666667\n",
       "21 21       0.6666667 0.66666667\n",
       "22 22       0.6666667 0.66666667\n",
       "23 23       0.5866667 0.33333333\n",
       "24 24       0.3200000 0.66666667\n",
       "25 25       0.6666667 0.66666667\n",
       "26 26       0.2800000 0.66666667\n",
       "27 27       0.6666667 0.66666667\n",
       "28 28       0.3066667 0.66666667\n",
       "29 29       0.4133333 0.66666667\n",
       "30 30       0.6000000 0.66666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta1_out = sgd(train_data, train_label, test_data, test_label, eta=0.01, epsilon=0.001, tau_max=100)\n",
    "eta2_out = sgd(train_data, train_label, test_data, test_label, eta=0.10, epsilon=0.001, tau_max=100)\n",
    "\n",
    "lim = min(nrow(eta1_out$error), nrow(eta2_out$error))\n",
    "error = data.frame(\"batch.id\"=1:lim, \"eta.01\"=eta1_out$error[1:lim,], \"eta.10\"=eta2_out$error[1:lim,])\n",
    "error_m = melt(error, id='batch.id')\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As we increase the training data, the model with the lower learning rate (0.01) can be seen slowly converging as opposed to the model with higher learning rate as it greedily tries to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
